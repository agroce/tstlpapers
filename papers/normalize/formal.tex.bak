\section{Formal Definitions}

%\subsection{A Brief Introduction to TSTL}

\begin{figure}[t]
{\scriptsize
\begin{code}
@import avl
\vspace{0.02in}
pool: <int> 4 CONST
pool: <avl> 3
\vspace{0.02in}
property: <avl>.check\_balanced()
\vspace{0.02in}
<int> := <[1..20]>
<avl> := avl.AVLTree()
\vspace{0.02in}
<avl>.insert(<int>)
<avl>.delete(<int>)
<avl>.find(<int>)
<avl>.inorder()
\end{code}
}
\caption{Part of TSTL harness for AVL trees.}
\label{fig:example}
\end{figure}

\mycomment{
\begin{figure}
{%\scriptsize
\begin{code}
avl1 = avl.AVLTree()  
int3 = 10  
int1 = 11  
avl1.insert(int1) 
int1 = 1  
avl1.insert(int3) 
avl1.insert(int1) 
int3 = 9  
avl1.insert(int3) 
int2 = 11  
avl1.delete(int2) 
\end{code}
}
\caption{An example TSTL-produced test}
\label{fig:avlrun}
\end{figure}
}

TSTL \cite{NFM15,ISSTA15,tstlsttt} is a language for defining the structure of
tests (usually API-call sequences, but also grammar-based tests), and a set of tools for use in generating,
manipulating, and understanding those tests.  Figure
\ref{fig:example} shows a simplified portion of a TSTL definition
(called a harness \cite{WODA08,WODACommon}) of
tests for an AVL tree class, in the latest syntax for TSTL.
%TSTL provides features not shown in this small example,
%including differential testing, customizable logging, support for
%complex guards, and use of pre- and post- values.  
Given a harness
like the one in Figure \ref{fig:example}, TSTL compiles it into a
class file defining an interface for testing that provides features
such as querying the set of available testing actions, restarting a
test, replaying a test, collecting code coverage data, and so forth.
The TSTL release \cite{tstl} provides testing tools that use the
interface for testing and debugging.

The key point for our purposes is merely that a TSTL test harness
defines a set of \emph{pools} that hold values produced and used
during testing \cite{AndrewsTR} (a common approach to defining
API-testing sequences) and a \emph{finite} set of actions that are possible during
testing, typically API calls and assignments to pool values.  In this
example, there are two pools, one named {\tt int} and one named {\tt
  avl}.  There are four instances of the {\tt int} pool, which means
that a test in progress can store up to 4 {\tt int}s at one time (in
variables named {\tt int0}, {\tt int1}, {\tt int2}, and {\tt int3}), and three
instances of the {\tt avl} pool.  The actions defined are: setting
the value of an {\tt int} pool to any integer in the range 1-20
inclusive, setting the value of an {\tt avl} pool to a newly
constructed AVL tree, and calling {\tt insert}, {\tt
  delete}, {\tt find} and {\tt inorder} with chosen pools.  Figure
\ref{threetests} in the introduction shows three
valid tests produced by running a random test generator on
the TSTL-compiled interface produced by this definition.  TSTL handles
ensuring that tests are well-formed. No pool instance
(such as {\tt avl1}) can appear in an action until it has been assigned
a value.  No pool instance that has been assigned a value can be
assigned a different value until it has been used in an action, to
avoid degenerate sequences such as {\tt int3 = 10} followed by {\tt
  int3 = 4}.  Each action in a test is called a ``step'' --- the
first step of the first test in Figure \ref{threetests} is storing a new AVL tree in {\tt
  avl0}, for example.  A test is just an ordered sequence of
actions, which is equivalent to a set of numbered steps.  Because our
normalization is defined in terms of actions, steps, and pools, it is
language agnostic.

The definition of pools and actions in TSTL defines a \emph{total
  order} on all actions.  First, actions are ordered by their position
in the harness definition file.  All {\tt insert} actions therefore precede
all {\tt delete} actions, and all {\tt delete} actions precede {\tt
  find} actions.  One line of TSTL typically defines more than one action. For
example, the line {\tt <avl>.insert(<int>)} defines 12 actions, one
for each choice of {\tt avl} and {\tt int} pool instance, e.g.
{\tt avl0.insert(int0)}, {\tt avl1.insert(int0)}, and {\tt
  avl0.insert(int1)}.  These are
ordered lexically, with the first pool appearing in the text taking
precedence.  Value ranges, such as in the {\tt int} initialization, are also
ordered in the natural way, with lower values first.  Given this total
order, each action can be assigned a unique index, from 0 up to 1 less
than the total number of actions. Initially, this kind of ordering (and
numbering) for each action was intended to allow for a kind of
G\"odel-numbering of tests, for use in proofs about
properties of test-generation algorithms
\cite{AndrewsTR}.

The ordering of actions also allows us to concisely define a
practical method for normalizing tests, by
simple syntactic means, by considering an action ``simpler'' than another
action if it has a lower index.  One intuitive concern with normalization
based on action ordering is that a user may not place actions
in a ``good'' order.  What if normalization
rewrites actions into ``more complex'' actions?
The exact ordering usually \emph{does not matter}, it only
matters that there exists \emph{some ordering} to guide normalization
and generalization.  Actions will only be replaced or swapped if,
causally, the predicate (failure) is \emph{indifferent} to the choice.  The
normal form's \emph{existence} matters much more than the precise contents, in
the context of that semantic restriction.  In cases where there is a
clear notion of simplicity (e.g., more vs. fewer optional parameters
to a call, fewer array dimensions), simplest-first is often
also most natural in TSTL.

\subsection{Normalization}

A test normalization algorithm has a simple goal:  we ideally aim to
produce a function $f : t \rightarrow t$ (a function that takes a test
and returns a test) such that: (1) if $t$ fails, $f(t)$ fails\footnote{Normalization can
    be generalized to apply to any predicate over tests, not just
    failure \cite{icst2014}; a passing test can be normalized, though
    without some more interesting predicate, such as preservation of
    coverage, this is not clearly useful.}; (2) if $t_1$ and $t_2$ fail
  due to the same fault, $f(t_1) = f(t_2)$; (3) if $t_1$ and $t_2$ fail due to different faults, $f(t_1) \not=
  f(t_2)$.

\begin{comment}
\begin{enumerate}
\item If $t$ fails, $f(t)$ fails\footnote{Normalization can
    be generalized to apply to any predicate over tests, not just
    failure \cite{icst2014}.}.
\item If $t_1$ and $t_2$ fail due to the same fault, $f(t_1) = f(t_2)$.
\item If $t_1$ and $t_2$ fail due to different faults, $f(t_1) \not=
  f(t_2)$.
\end{enumerate}
\end{comment}

Such a function would define a true \emph{canonical form} for tests, where each underlying fault is uniquely represented by a single
test.  In general, it seems clear that defining such a function
$f$ is (at least) as difficult as automatic fault localization and
repair, and thus undecidable.  Therefore, we aim at approximating the
goal, by providing a set of simple transformations such that: (1) $f$
changes many tests to the same test, (2) $f$ has low probability of
changing two tests failing for different reasons into the same test,
and (3) $f$ is not unreasonably expensive to compute.  The implementation
for $f$ (in fact, for a family of $f$-approximating functions, with
different tradeoffs in runtime and level of normalization) involves
defining a set of rewrite rules such that for a test $t$, the rules
define a finite set of candidate tests $C(t)$ where $t' \in C(t)$  if $t \Rightarrow t'$, possible
simplifications of $t$, where each $t'$ is the result of applying some
rewrite rule to $t$.  The notion of simplicity is defined by a
restriction on the rewriting rules.  For any rewrite $t \Rightarrow t'$, we require that $|C(t')| < |C(t)|$.  Such
a rewrite system is necessarily \emph{strongly normalizing}: any
sequence of rewrites chosen will eventually end in a term (test)
that cannot be further rewritten, since the total length of a rewrite
sequence is bounded by the initial $|C(t)|$ \cite{term1}.

\mycomment{, let $R(t)$ be the length of the maximum number of
rewrites that can be applied to $t$, e.g., the longest possible
sequence such that $c_0 = r_{i1}(t), c_1 = r_{i2}(c_0), ... c_n = r_{in}(c_{n-1})$. We
require that $\forall c \in C(b), R(c) < R(b)$.  The number of possible
rewrites for each candidate must be smaller than the number of
possible rewrites for $b$.  This implies further restrictions, e.g., no
rewrite can ever reverse another rewrite's effects.  Such a rewrite
system is \emph{strongly normalizing}:  any sequence of rewrites
chosen will eventually end in a term (test) that cannot be
further rewritten.}

In the setting of TSTL, where test actions have a defined total
order, two simple principles can be applied to produce useful
rewrite rules.  All rewrites reduce the sum of the
indices of the actions in the test, make the test's
actions more ordered by index, or reduce test length.  This guarantees that the rewrites are
strongly normalizing.
The second principle that determines the rewrite rules is that each
rule ought to be unlikely to change the underlying cause for test
failure.  To that end, the rules for normalization always either change at most
one action (possibly in multiple steps, but in a uniform way) or make
\emph{no} changes to the actions performed, only to the pools used or the
positions of actions in the test.  We cannot guarantee
normalization does not change the underlying fault in a test; however,
the limited scope of rewrites should minimize the chance of fault
change (known as ``slippage'' \cite{PLDI13,slippage}).

\subsubsection{Rewrite Rules}

Figure \ref{fig:rewrite} shows the rewrite rules used in TSTL normalization.
The notation in the rules is relatively simple.
A \emph{step} is an action paired with an index indicating its
position in a test,
where the first action is step 0, etc.; e.g., $(2: a)$ indicates the
third step of the test is action $a$ (indexing is from 0). 
$\Delta(t,t')$ is the set of all steps in $t$ such that $t(i) \not= t'(i)$.

For ordering, we say that
$a < b$ iff the index of action $a$ is lower
than that of action $b$.  We compare steps with $<$ by comparing their
actions --- $(i: a) < (j: b)$ iff $a < b$.  For a set or sequence of actions or steps, we define the $min$ of the
set to be the lowest indexed action in the set, and use
these to compare sets:  $s_1 < s_2$ iff $min(s_1) < min(s_2)$. For pools,
$p < p'$ if and only if $p$'s index is lower than the index for $p'$
\emph{and} $p$ and $p'$ are from the same pool.

The term $t[x \mapsto y]$ denotes the test t with all instances of $x$
replaced by $y$.  Here, $x$ and $y$ can be actions, steps, or pools.
$t[x \leftrightarrow y]$ is similar, except that $x$ and $y$ are
swapped.  Term $t(i,j)[x \mapsto y]$ is the same as $t[x \mapsto
y]$, except that the replacement is only applied between steps $i$ and
$j$, inclusive.  Finally, $t_{\rightarrow i}(x)$ denotes $t$ with all steps
containing $x$ that are before step $i$ moved to step $i$, preserving
their previous order, shifting steps at $i$ and after $i$ to make room for
the moved steps, again preserving order.

%\subsubsection{Rewrite Rules}

\begin{figure}[t]
{%\scriptsize
\begin{itemize}
\item {\bf SimplifyAll:}
$t \Rightarrow t[a \mapsto a']$\\
\-\ \ \ \emph{where} $a' < a$\\
Covers the case where all appearances of an action can be replaced with a 
lower-indexed action. 
\item {\bf ReplacePool:}
$t \Rightarrow t(i,j)[p \mapsto p']$\\ 
\-\ \ \ \emph{where} $p < p'$ and $0 \leq i < j <
|t|$\\
Covers the case when all appearances of an instance of a pool can be replaced with 
a lower-indexed instance of that pool (possibly only within a range of steps).
\item {\bf ReplaceMovePool:}
$t \Rightarrow t_{\rightarrow i}(p')[p \mapsto p']$\\
\-\ \ \ \emph{where} $p < p'$ and $0
\leq i < |t|$\\
Covers the case when all appearances of an instance of a pool can be replaced with
a lower-indexed instance of that pool, if all steps containing the new instance before a
certain step are then moved to that step.
\item {\bf SimplifySingle:}
$t \Rightarrow t[(i: a) \mapsto (i: a')]$\\
\-\ \ \ \emph{where} $a' < a$\\
Covers the case where one action can be replaced with a 
simpler (lower-indexed) action. 
\item {\bf SwapPool:}
$t \Rightarrow t(i,j)[p \Leftrightarrow p']$\\
\-\ \ \ \emph{where} $\Delta(t',t) < \Delta(t,t')$\\
\-\ \ \ and $0 \leq i < j < |t|$\\
\-\ \ \ and $p < p'$\\
Covers the case where swapping two pool instances (within a range of steps) reduces
the minimum action index of the steps.
\item {\bf SwapAction:}
$t \Rightarrow t[(i: a) \mapsto (i: b), (j: b) \mapsto (j: a)]$\\
\-\ \ \ \emph{where} $i < j$ and
$b < a$\\
Covers the case where two actions can be swapped in the test, with the
lower-indexed action appearing first.
\item {\bf ReduceAction:}
$t \Rightarrow t[(i: a) \mapsto (i: a')]$\\
\-\ \ \ \emph{where} $|ddmin(t')| < |t|$\\
Covers the case where an action can be replaced by any action, enabling further delta-debugging.
\end{itemize}
}
\caption{Rewrite rules for normalization.}
\label{fig:rewrite}
\end{figure}

\subsubsection{Normalization Algorithm}
\label{formalexample}

These rules alone do not determine a complete normalization method; it is
also necessary to determine the order in which they are applied.  The
order in our default implementation is the order above, with the
modification that in practice the {\bf ReplacePool} and {\bf
  ReplaceMovePool} rewrites are checked in the same loop
(e.g., for every possible replacement of a pool, both rules are
checked, in the order given above).  The core algorithm, assuming a set
of ordered rewrite rules defines $C(t)$, is given as Algorithm
\ref{simpalg}.  Here {\tt pred} is an arbitrary predicate indicating
that the candidate test still satisfies the property of interest that
held for the original test $t$.  In most cases, this predicate will be
``the test fails'' but we also have preserved
code coverage for regression suites \cite{icst2014}.  Notice that
after applying each rewrite rule, we perform delta-debugging on the
new base test, since often a rewrite makes other steps irrelevant.

\begin{algorithm}
\caption{Basic algorithm for normalization}
\label{simpalg}
{%\scriptsize
\begin{algorithmic}[1]
\State {modified = True}
\While {modified}
\State {modified = False}
\For {$t' \in C(t)$}
\If{pred($t'$)}
\State modified = True 
\State $t$ = $ddmin(t')$
\State {\bf break} (exit {\bf for} loop) 
\EndIf 
\EndFor 
\EndWhile 
\State return $t$
\end{algorithmic}
}
\end{algorithm}

\begin{figure}
\raggedright
{\bf Original test:}\hspace{1.5in}{\bf Normalized:}
{\scriptsize
\begin{verbatim}
  0: int0 = 10                                    0: int0 = 1
  1: int2 = 7                                     1: int1 = 2
  2: avl1 = avl.AVLTree()                         2: avl0 = avl.AVLTree()
  3: avl1.insert(int2)                            3: avl0.insert(int0) 
  4: avl1.insert(int0)                            4: avl0.insert(int1) 
  5: int1 = 1                                     5: int1 = 3
  6: int3 = 1                                     6: avl0.insert(int1) 
  7: avl1.insert(int3)                            7: int1 = 4
  8: int3 = 15                                    8: avl0.insert(int1)
  9: avl1.insert(int3)                            9: avl0.delete(int0)
 10: avl1.delete(int1) 
\end{verbatim}
}
{\bf Normalization Steps:}
\raggedright
\begin{code}
{\bf SimplifyAll}: int0 = 10 $\mapsto$ int0 = 2 
{\bf SimplifyAll}: int2 = 7  $\mapsto$ int2 = 3 
{\bf SimplifyAll}: int3 = 15  $\mapsto$ int3 = 4 
{\bf ReplacePool}: int2 $\mapsto$ int1
{\bf ReplacePool}: avl1 $\mapsto$ avl0
{\bf ReplacePool}: int3 $\mapsto$ int0
{\bf SwapAction}: (0: int0 = 2)  $\leftrightarrow$ (6: int0 = 1)
{\bf SwapPool}: int0 $\leftrightarrow$ int1 (between steps 2 and 10)
{\bf SwapAction}: (1: int1 = 3)  $\leftrightarrow$ (5: int1 = 2)
\end{code}

\caption{An example of normalization steps.}
\label{diffnorm}
\end{figure}

Figure \ref{diffnorm} shows the sequence of rewrites to
normalize an AVL tree failure.  The numbering
of steps appears inconsistent in the first {\bf SwapAction} because a
successful delta-debugging removes a no-longer-needed step after a
rewrite.  The pattern in this example, where successful rewrites
are roughly equal to the original number of steps, is frequently seen across SUTs.

The worst-case complexity of normalization can be given an upper bound by recalling our
rule that each rewrite must lower the number of possible rewrites by
at least one.  This means if there are $n$ possible rewrites of a
test, there can be at most $n$ predicate checks for the current test,
then at most $n-1$ checks for the rewritten test, and so on, for
a total of $\frac{n(n+1)}{2}$ predicate checks, where $n$ is the
number of possible rewrites of the test $t$ being normalized:
$n = |C(t)|$.  Test execution to check the predicate can be
assumed to have a constant cost since the length of the test does
not usually change by more than a few steps during normalization. 

How many rewrites can a test $t$ have?  Assume there are $k$
steps in the test and $\alpha$ possible actions.  The action
replacement rewrites ({\bf SimplifyAll}, {\bf SimplifySingle}, and
{\bf ReduceAction}) allow for at most $k (\alpha-1)$ rewrites each.  There
are $\leq k^2$ possible {\bf SwapAction} rewrites, and $\leq k^2(\alpha-1)$
possible rewrites for each of {\bf ReplacePool}, {\bf
  ReplaceMovePool}, and {\bf SwapPool}\footnote{The details of how
  these bounds are determined, in that pool changes are also action
  changes, are not critical, and a more detailed analysis is somewhat
  involved, and beyond the scope of this paper; we note that like
  delta-debugging, the worst-case complexity is seldom observed, and
  offer some further optimizations.}.  There are therefore at
most $n=k^2 + 3k^2(\alpha-1) + 3k(\alpha-1)$ rewrites, over-approximating (since
an action cannot be rewritten to itself).  Each rewrite also
requires a \emph{ddmin} call, which is quadratic in $k$ \cite{DD}.
Substituting this expression into the expression $\frac{n(n+1)}{2}$
above, we see that the worst-case cost for normalization is
$O(k^4\alpha^2)$.  While this is
worse than the quadratic cost of delta-debugging, this algorithm is
applied to already 1-minimal tests, unlike delta-debugging. The $k$ in our quartic complexity
is therefore, for random tests, typically an order of magnitude smaller
than the $k$ in delta-debugging \cite{ICSEDiff,icst2014,MinUnit},
which can partly balance the additional cost for tests whose
unreduced length is less than 100.

\mycomment{
checks (we assume a constant cost to run a
check --- unlike in delta-debugging, this is a reasonable assumption, as few
rules change the length of the test).  Each time the inner loop finds
a valid rewrite, there is also the cost of a delta-debugging step,
which is at worst quadratic in $k$ \cite{DD}.  The cost of the inner
loop is therefore $O(k^2n)$.  How many times can this inner loop run
(e.g., how many times can the {\bf while} loop run)?  Each
action can be replaced fewer than $n$ times, since each
replacement (or at least one replacement in each rewrite,
for the pool changes) must decrease the index of the action.  Steps
can be swapped fewer than $k$ times, obviously.  Each  {\bf
  ReduceAction} rewrite requires reducing the length of the
test, giving it a total (not per-step, like the other bounds) bound of
$k$ successful rewrites.  This gives us an upper bound of $kn +
k^2 + k$ executions for the outer loop, and a total runtime that is
$O(k^4n)$.}

We believe that if normalization can decrease human effort in
examining large numbers of redundant tests, this price is more
than reasonable.  In practice, most actions are not enabled at most
steps, and the rules are applied in an order that quickly converges on
a normal form for many tests.  In our experiments, normalization
was only very expensive when delta-debugging was also costly, and
appeared to be worse than delta-debugging by a constant factor,
somewhere in the range of 2-100x, usually close to 10x.  With multiple
faults, normalization provides a quantifiable value in
shorter time until all faults have been examined \cite{PLDI13}.

%\subsubsection{Normalization Optimizations}

The simplest optimization is to improve on the constant ordering of
rewrite rules.  Once a rule fails to produce a candidate that
satisfies {\tt pred}, that rule should be moved to the end of the
ordering of rewrites, since once a rule fails once to produce any
valid rewrites, it frequently produces no further reductions.  This
simple change typically halves the time required for normalization.
When test execution is very expensive, the set of candidate tests can be further restricted: limiting action replacements to cases
where Levenshtein \cite{Lev} distance (text edit distance) between the
code for actions is bounded to a small value was effective in
reducing runtime, and often had little impact on final results.
A further useful optimization when normalizing large numbers of tests
is to cache results across tests (since the algorithm is deterministic
for a given {\tt pred}).   For very large
numbers of tests, this is the most important optimization.
For systems with
expensive replay, delta-debugging of each new base test
can be omitted: the {\bf ReduceAction} rewrite will
eventually remove extraneous steps.  
It is also trivial to parallelize normalization by
checking the predicate over multiple candidates at once.  As soon as a
candidate satisfies the predicate, a parallel implementation can proceed with that candidate as the new
base, making the algorithm nondeterministic (in practice, we suspect
the same final result will usually appear), or the algorithm can wait
for all earlier-in-sequence candidates to be checked, and only proceed
when no candidate that would be checked earlier is in the queue.

%\subsection{Normalization Example}




\subsection{Generalization}



The core idea of generalization is to use methods similar to those
involved in normalization to provide a user with information about
changeable aspects of a test.  Some values and orderings of steps
in a test are \emph{essential} to the failure: when changed, they
cause the test to no longer fail.  Many others, however, are
\emph{accidental} --- any concrete test has to choose \emph{some}
values and step ordering (enforced by the normalization
process) but many such choices are arbitrary, or at least allow
variance, with respect to the cause of failure.  Generalization
automatically performs experiments to distinguish essential and accidental aspects
of a test, and summarizes the results.  We
can define a finite set of simple variations on a test, and summarize which
variations maintain a predicate of interest. In the typical case of
failure, generalization simply presents the user with \emph{all}
``very similar'' tests that also fail. Formally
speaking, a generalization algorithm produces as output \emph{a set of
  rewrites} of a test $t$ satisfying {\tt pred} such that each rewrite
also satisfies {\tt pred}.  Presenting a generalization involves
concisely expressing these (small) rewrites.

\subsubsection{Generalization Algorithm}

The core algorithm
(Algorithm \ref{genalg}) is simple, using only swaps and single-action
rewrites:

\begin{algorithm}
\caption{Basic algorithm for generalization}
\label{genalg}
{%\scriptsize
\begin{algorithmic}[1]
\State {swap = $\emptyset$}
\State {replace = $\emptyset$}
\For {$(i, a) \in t$}
\For {$a' : a' > a$}:
\If {pred($t[(i,a) \mapsto (i,a')]$)}
\State replace = replace $\cup ((i,a),(i,a'))$
\EndIf
\EndFor 
\For{$j : i < j < |t|-1 \wedge (j: b) > (i: a) $}
\If {pred($t[(i: a) \mapsto (i: b), (j: b) \mapsto (j: a)]$)}
\State swap = swap $\cup ((i,a),(j,b))$
\EndIf
\EndFor
\EndFor
\State {return (swap, replace)}
\end{algorithmic}
}
\end{algorithm}

This algorithm collects all steps that can be replaced with other
actions or swapped with other steps, and returns the set to be
reported to the user.  This version assumes the test has already been
normalized, but can be extended to any test by removing the
restrictions that $a > a'$ and $(j : b) > (i : a)$.  The complexity of
generalization is simpler to determine than that of normalization.  If
we assume all actions are enabled at each step, and there are $\alpha$
actions and $k$ steps, checking for replacements requires $k (\alpha-1)$
test executions, when every action is the lowest-indexed action.
In that worst case, no swaps are possible.  The complexity of checking
for swaps in the worst case is quadratic in $k$.  In practice, most
actions are not enabled at most steps, and most actions in a test
are not the lowest-indexed action.  Basic generalization is trivial to
parallelize.


\subsubsection{Fresh Values and Misleading Tests}
\label{freshgen}

A side-effect of delta-debugging and normalization is reduction of the
number of variables in a test.  While usually helpful, this can
sometimes result in misleading tests.  In a stateful system,
putting the system into a bad state may require building a complex
object.  Once system state is corrupted, however, the complex object
is irrelevant, and its appearance in the call leading to failure can
be misleading.  In previous work at NASA, we observed that sometimes a
delta-debugged file system test \cite{ICSEDiff,AMAI} would use an
open file descriptor in a call, leading to the suspicion that the file
had been corrupted, when in fact the file system's state was damaged,
and the same operation on \emph{any} file would have failed.
We therefore propose a more aggressive generalization: replacing a pool use with a \emph{fresh value}.

Consider the test in Figure \ref{fig:mislead}, produced by a
TSTL harness for  TSTL itself\footnote{Since Python TSTL
  provides a Python API, that API can be used as the SUT in testing.}.  The
problem involves an invalid cache, produced by normalizing a test with
only one action.  Without fresh value generalization, it appears that
the failure is due to normalizing {\tt test0} again.
The annotation after step 6 lets us see that the failure
will take place even for a fresh test.
Without this generalization, the state of {\tt test} may
appear to be important, not the system state.

\begin{figure}
{\scriptsize
\begin{code}
\textcolor{black!60}{\#[}
test0 = []                             \textcolor{black!60}{\# STEP 0}
\textcolor{black!60}{\#  or test0 = sut0.test() }
actionlist0 = sut0.actions()           \textcolor{black!60}{\# STEP 1}
\textcolor{black!60}{\#  or actionlist0 = sut0.enabled() }
\textcolor{black!60}{\#] (steps in [] can be in any order)}
action0 = actionlist0[0]               \textcolor{black!60}{\# STEP 2}
\textcolor{black!60}{\#[}
test0.append(action0)                  \textcolor{black!60}{\# STEP 3}
pred0 = sut0.fails                     \textcolor{black!60}{\# STEP 4}
\textcolor{black!60}{\#] (steps in [] can be in any order)}
sut0.normalize(test0,pred0)            \textcolor{black!60}{\# STEP 5}
sut0.normalize(test0,pred0)            \textcolor{black!60}{\# STEP 6}
\textcolor{black!60}{\#  or (}
\textcolor{black!60}{\#      test0 = []  ;}
\textcolor{black!60}{\#      sut0.normalize(test0,pred0) }
\textcolor{black!60}{\#     )}
\end{code}
}
\caption{Generalized test for TSTL itself, showing fresh value
  generalization.}
\label{fig:mislead}
\end{figure}

Formalizing this generalization requires additional notation.
$U(a)$ is the set of pools \emph{used} in the action $a$ --- pools that
in the action, but not on the left-hand side of an assignment.
$I(a,p)$ is a predicate that is true iff action $a$ stores a new value
in pool $p$.  Finally, $t[+(a: i)]$ denotes test $t$ with the action
$a$ inserted at step $i$ and each step from $i$ onwards moved to a
position one higher.

\begin{algorithm}
\caption{Basic algorithm for fresh object generalization}
\label{freshalg}
{%\scriptsize
\begin{algorithmic}[1]
\State {fresh = $\emptyset$}
\For {$(i, a) \in t$}
\For{$p \in U(a)$}
\For{$a' : I(a',p)$}
\If {pred($t[+(a',i)]$)}
\State fresh = fresh $\cup (i,a')$
\EndIf
\EndFor 
\EndFor
\EndFor
\State {return fresh}
\end{algorithmic}
}
\end{algorithm}

In practice, the {\tt fresh} set returned should be pruned to avoid
redundant actions.  It is not useful information that {\tt int0 = 1 ; int1 = 2; int0 = 1; f(int0)} fails if {\tt int0 = 1;
  int1 = 2; f(int0)} fails.  Redundancy elimination also needs to take
into account the potential assignments to a pool from the {\tt
  replace} generalization, which are also redundant.  Furthermore, it
is useful to distinguish between pools that are never modified, only
assigned to, and pools that are modified without appearing on a
left-hand side (LHS).  As an example, if an integer is used as an argument
to a function, the pool value's last assignment is still valid and
should be omitted from ``fresh'' values, as redundant.  However,
calling a function on an AVL tree may modify it, making an assignment
non-redundant, even if it is the last appearance of that pool on the
LHS.
We use the {\tt CONST} tag (see Figure \ref{fig:example}) to mark
values than cannot be modified on the RHS.  Further extensions of the
fresh value generalization could be considered.  For example, if a
fresh value for some object requires use of a complex constructor,
values required to call the constructor can also be produced, if
needed, recursively.  In our experiments so
far, simple fresh value generation sufficed, as inputs to
constructors were usually available in pools.  Knowing all possible
fresh values is likely unimportant.

%\subsection{Discussion}

