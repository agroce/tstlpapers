\section{Introduction}

Software test automation encompasses two challenges: (1) automated
execution and determination of results for human-created tests, and
(2) truly automatic generation of tests.  Both are critical for effective,
efficient software testing, but only test generation offers the
potential to discover faults without human determination that a
particular execution scenario has the potential to behave incorrectly.
Automated generation of tests relies on the construction of \emph{test
  harnesses}.  A \emph{test harness} defines the set of valid tests
(and, usually, a set of correctness properties for those tests) for
the Software Under Test (SUT).  This paper presents a language and
tools applying insights from the world of explicit-state model
checking to the problem of producing test harnesses for automated
test generation, whether tests are produced by a exhaustive
state-space exploration as in model checking, or via less
systematic methods.

Building a test harness is a task that even experts in
model checking and automated testing often find painful
\cite{woda08,woda12}.  The difficulty of harness generation is one
reason for the limited adoption of automated testing and model
checking methods by the typical developer who writes unit tests.  This is
unfortunate, as even simple random testing can often uncover subtle
faults.

The ``natural'' way to write a test harness is as code in the language
of the SUT.  This is obviously how most unit
tests are written, as witnessed by the proliferation of tools like
JUnit \cite{JUnit} and its imitators (e.g., PyUnit, HUnit, etc.).  It
is also how many industrial-strength random testing systems are
written \cite{ICSEDiff,AMAI}.  A KLEE ``test harness'' \cite{KLEE} for
symbolic execution is written in C, with a few additional constructs
to indicate which values are symbolic.  This approach is common in
model checking as well: e.g., Java Pathfinder \cite{JPF,JPF2} can
easily be seen as offering a way to define a state space using Java
itself as the modeling language, and CBMC \cite{CBMC,CBMCp} performs a
similar function in C, using SAT/SMT-based bounded model checking
instead of explicit-state execution.  JPF in particular has shown how
writing a harness in the SUT's own language can make it easy to
perform ``apples to apples'' comparisons of various testing/model
checking strategies \cite{JPFRandTest}.


\begin{figure}[b]
{\scriptsize
\begin{code}
op = choice(operations);
val1 = choice(values);
val2 = choice(values);
if (op == op1 \&\& guard1) \{
    call1(val1);
\} else if (op == op2 \&\& guard2) \{
    call2(val1,val2);
\} else if (op == op3 \&\& guard3) \{
    call3(val1,val2);
...
\end{code}
}
\vspace{-0.15in}
\caption {A test harness in the SUT's language.}
\label{fig:badharness}
\end{figure}

Unfortunately, writing test harnesses this way is a highly repetitive
and error-prone programming task, with many conceptual ``code clones''
(e.g. Figure \ref{fig:badharness}). A user faces difficult choices in
constructing such a harness. For example, the example harness always assigns {\tt val2} even
though {\tt call1} only uses {\tt val1}, to avoid having to repeat the
choice code for calls 2 and 3.  The harness is almost certainly
sub-optimal for  random testing, where the lack of any
memory for previously chosen values can make it hard to exercise code
behaviors that rely on providing the same arguments to multiple method
calls (e.g., {\tt insert} and {\tt delete} for container classes).
The construction of a harness becomes even more complex in realistic
cases, where the tested behaviors involve building up complex types as
inputs to method calls, rather than simple integer choices. For
example, consider the problem of testing a complex Python library.  Figure
\ref{fig:MakeFeatureLayer} shows a portion of the Python documentation
for one function in the ArcPy \cite{ArcPy} site
package for Geographic Information Systems (GIS) automation.  Rather than taking a single integer, this
function call requires complex inputs --- a feature class or
layer, an SQL expression, and other complex types that we can assume
are also difficult to construct.  A harness testing a typical
real-world library must manage the creation of values of many such complex types.
Moreover, because building up function inputs is itself complicated
and requires complex method calls, these values cannot simply be produced on each iteration, but must be
stored and selected for use in future calls.
The code quickly becomes hard to read, hard to maintain, and hard to
debug.  In some cases \cite{AMAI} the code for a sophisticated test
harness approaches the SUT in complexity and even size!  The code's
structure also tends to lock in many choices that would ideally be configurable.

\begin{figure}[t]
{\scriptsize
\begin{code}
MakeFeatureLayer\_management(in\_features, out\_layer, {where\_clause}, {workspace}, {field\_info})
   
   Creates a feature layer from an input feature class or layer file. The layer
   that is created by the tool is temporary and will not persist after the session
   ends unless the layer is saved to disk or the map document is saved.
    
INPUTS:
 in\_features (Feature Layer):
   The input feature class or layer from which to make the new layer. Complex
   feature classes, such as annotation and dimensions, are not valid inputs to this tool.
 where\_clause \{SQL Expression\}:
   An SQL expression used to select a subset of features. For more information on
   SQL syntax see the help topic SQL reference for query expressions used in ArcGIS.
...
\end{code}
}
\caption{Documentation for a function in Esri's ArcPy site package.}
\label{fig:MakeFeatureLayer}
\end{figure}

One of the most important of these locked-in choices is the  test generation
method.  Writing a harness by hand usually makes it hard to try out
new strategies.   Writing novel testing strategies in even such
an extensible platform as Java Pathfinder is hardly a task for the
non-expert.
The harness in Figure \ref{fig:badharness} may support random testing and
some form of model checking, if it is written in Java and can use JPF
or a library for adaptation-based testing \cite{ISSRE12}. Such a
harness will likely be completely inflexible as to generation method if written in Python, C,
or another language without that level of tool support.

What the user really wants is to simply provide a concise version of the information in
Figure \ref{fig:MakeFeatureLayer}, some configuration details (e.g., how many
feature classes to keep track of at once), and then try different test
generation methods.  While some automated testing tools for Java \cite{FA11,Pacheco}
can automatically extract
method signatures from source code and produces tests,  
using such a tool locks a user into one test generation method.
Completely automatic extraction also often fails to handle
the subtle details of harness construction, such as defining guards
for some operations, or temporal constraints between API calls that
are not detectable by simple exception behavior.  Understanding
problems with automatic extraction can be hard with large libraries,
since the extraction tends to either produce internal data structures
only or produces a huge, impenetrable mass of code. The user \emph{wants} a
declarative harness, but often \emph{needs} to program critical details of a
harness, and build understanding of the system by performing harness development in
small, incremental steps.

\subsection{Contributions}

In this paper we describe a complete, Domain Specific Language (DSL)-based approach that combines
a simple means to produce a declarative harness with the full power of
a complete programming language.  TSTL (the Template Scripting Testing
Language) compiles a declarative description of system state and
actions into a library in the language of the System Under Test
(SUT).  This library allows the creation of objects providing an API
for testing the SUT, including support for state comparison,
abstraction, backtracking, automatic test case reduction, code coverage, and support
for sophisticated regression testing.

Using an ongoing case study,
we show how to apply TSTL and its tool suite to a large, real-world software
library used in critical applications.  The test effort has been driven and directed not by a
software testing researcher (as is the usual case), but by a domain
expert in the Geographic Information Systems (GIS) SUT.  In the
course of this effort, multiple faults and undocumented restrictions of the library
under test have been discovered, and the TSTL language and tool suite have
been transformed from a research prototype into a complete system for
software testing.

This paper presents the most complete presentation of the TSTL
language and tools, and we hope that it satisfies three critical goals:

\begin{itemize}
\item First, would-be users wanting to take advantage of automated
  test generation should be able to base their own testing
  efforts using TSTL on the example code in this paper (and that available
  in the TSTL github repository \cite{tstl}).
  This paper thus completely describes the concepts behind TSTL, the
  semantics of the language, and the tools available in TSTL.
  Previous papers on TSTL \cite{NFM15,ISSTA15} reported a much less full-featured version of the
  language using a difficult-to-read syntax.

\item Second, researchers should be able to use the information in this paper to
  extend existing TSTL tools or build their own tools to explore novel
  test generation strategies, automated debugging methods, and other
  research prototypes.  TSTL enables easy comparison of
  methods in a framework reducing the burden of implementation
  and avoiding irrelevant differences in performance due to underlying
  infrastructure.  The growing set of SUTs
  included in the TSTL distribution, which includes large and widely
  used Python libraries, can provide benchmarks for
  experimental efforts.  

\item Finally, unlike previous publications on TSTL, this paper
  emphasizes the fact that TSTL, unlike other testing DSLs or tools,
  at heart transforms a definition of valid tests (and properties) for
  a System Under Test into a \emph{programming language interface} for testing
  that system.  Tests in TSTL are not inaccessible entities internal to
  a tool,
  or only represented as unit tests (i.e., programs) that cannot be
  easily manipulated and analyzed, but first-class objects in the
  language of the System Under Test.  To our knowledge, this approach
  to testing has not been previously explored, and it was not
  emphasized (or even clearly presented) in earlier publications on TSTL.
\end{itemize}


The organization of this paper is as follows.  In Section
\ref{sec:dsltest} we present the basic idea of a DSL for testing, and
distinguish TSTL from other testing DSLs.  Section \ref{sec:arcpy} 
provides background on the ArcPy GIS case study used throughout the
paper.  Section \ref{sec:lang} provides a full description, with
examples, of the core TSTL language and semantics.  Section
\ref{sec:tools} describes the tools included with TSTL, and
Section \ref{sec:build} describes how researchers and developers can
build their own TSTL-based testing tools to support additional
testing, debugging, or regression strategies.  Section
\ref{sec:langext} introduces the novel TSTL concept of making testing
a first-class activity in a programming language, similar to how other
libraries make GIS (ArcPy), scientific computing (NumPy \cite{NumPy},
SciPy \cite{SciPy}) or bioinformatics
(QIIME \cite{QIIME}, Biopython \cite{biopython}, scikit-bio \cite{scikitbio}) activities simple to use in either a scripted or
interactive manner.  Faults discovered
in TSTL, in ArcPy and other systems, are described briefly in Section
\ref{sec:bugs}.  We survey the most closely related work in Section
\ref{sec:related}, and summarize our conclusions in Section \ref{sec:conclusion}.

\section{Domain Specific Languages for Testing}
\label{sec:dsltest}

The nature of test harness construction suggests the use of a
\emph{domain-specific language} (DSL) for testing \cite{ISOLA12}.  DSLs
\cite{Fow10} provide abstractions and notations to support a
particular programming domain. The use of DSLs is a formalization of
the long-standing approach of using ``little languages,'' as advocated by Jon Bentley in a
Programming Pearls column \cite{LitLang} and exemplified in such system
designs as UNIX.  DSLs typically come in two forms: \emph{external}
and \emph{internal}.  An external DSL is a stand-alone language, with
its own syntax.  An internal DSL, also known as a domain-specific
embedded language (DSEL), is hosted in a full-featured programming
language, restricting it to the syntax (and semantics) of that
language.  Many attempts to define harnesses can be seen as internal
DSLs \cite{UDITA,ISSRE12,JPF2,CBMCp,KLEE}.  Neither of these choices
is quite right for test harnesses.  Simply adding operations for
nondeterministic choice still leaves most of
the tedious work of harness definition to the user, and makes changing
testing approaches difficult.  With an external DSL, the user
must learn a new language, and the easier it is to learn, the less
likely it is to support the full range of features needed.

A novel approach is taken in recent versions of the SPIN model checker
\cite{SPIN}.  Version 4.0 of SPIN \cite{ModelDriven} exploited the
fact that SPIN works by producing a C program from a PROMELA model
to allow users to include calls to the C language in their PROMELA models.  The
ability to directly call C code makes it much easier to model check
large, complex C programs \cite{AMAI,ModelCode}.  C serves as a
``DSEL'' for SPIN, except that, rather than having a domain-specific
language inside a general-purpose one, here the domain-specific
language hosts a general-purpose language.  A similar embedding is
used in {\tt where} clauses of the LogScope language for testing Mars
Science Laboratory software \cite{scriptstospecs}.  We adopt this
approach for our own language and embed the general-purpose language (for expressiveness) in a
DSL (for concision and ease-of-use).

The most significant difference between TSTL and other DSLs for
testing and verification, including SPIN, is that most such systems
are primarily intended to be used as stand-alone tools.  Whether model
checkers \cite{SPIN,JPF2}, model-based testing tools \cite{Taxonomy},
or random testing tools \cite{Pacheco}, these systems are primarily
designed as ``things to \emph{run on} the system under test.''  TSTL
can operate in this manner, but at heart it transforms a definition of
valid tests into a library for creating, executing, manipulating, and
analyzing test cases.  An experienced TSTL user can interact with TSTL
at an interactive command prompt in the language of the SUT, creating,
saving, and modifying tests on-the-fly.  TSTL tools are simply
scripted formalizations of this mode of use, automating repetitive
tasks.  Such an approach is not possible with any other tool of which we
are aware.  Many tasks that are constrained to the functionality provided
by  tools included in other systems (e.g., replay of regression tests) in
TSTL are simplified and made flexible by this approach.

\subsection{TSTL: The Template Scripting Testing Language}

TSTL is based on understanding a test harness as a declaration
  of the possible actions the SUT can take, where these actions are
defined in the language of the SUT itself, with the full power
of the programming language to define guards, perform
pre-processing, and implement oracles.  Our particular approach is
based on what we call \emph{template scripting}.

The \emph{template} part of the name captures the fact that our method
proceeds by processing a harness definition file to output code that
enables testing, much  as SPIN processes PROMELA/C.  The harness
description file consists of fragments of code in the SUT's language
that are expanded, via the TSTL compiler, into a class that allows
an independently written test generation or manipulation tool to
generate, execute, or replay tests, without knowing
any details of the SUT.
A TSTL harness defines a \emph{template} for action definition, and
the compiler 
instantiates the template exhaustively.  The \emph{scripting} aspect indicates
TSTL is designed to be very lightweight and as easy for
users to pick up as a popular scripting language.  TSTL
works best when the SUT language is very
concise, like most scripting languages, making ``one-liners'' of action
definition possible; our initial implementation \cite{tstl} is therefore for
Python\footnote{We also have released a beta version of TSTL for Java \cite{TSTLJava}, to show
 that testing code in non-scripting languages is also possible.}.

\begin{figure}
{\scriptsize
\begin{code}
@from arcpy import *
\vspace{0.1in}
pools:
  <fc> 3 CONST             \# A feature class contains only lines, points, or polygons
  <newlayer> 3 CONST
  <op> 2 CONST 
  <val> 2 CONST
  <whereclause> 2 CONST    \# SQL clause to limit objects in new layers
  <fieldname> 2 CONST      \# Extracted from the shape files
  <fieldlist> 2
\vspace{0.1in}
actions:
\vspace{0.1in}
<fc> := <["d1.shp", "d2.shp", "d3.shp"]>  \# Just shapefiles for this example
<newlayer> := <["newl1", "newl2", "newl3"]>
\vspace{0.05in}
\{IOError\} <fieldlist> := ListFields(<fc>) \# Extract fields from a feature class
len(<fieldlist,1>) >= 1 -> <fieldname> := <fieldlist> [0].name 
<fieldlist> = <fieldlist> [1:] \# Skip to next field
\vspace{0.05in}
<op> := <[">", "<", "<=", ">=", "=", "!="]>
<val> := <1..10>
<val> = <val> * 10
<val> = <val> + 1

<whereclause> := '"' + <fieldname> + '" ' + <op> + str(<val>)
<whereclause> = <whereclause> + ' AND ' + <whereclause>
<whereclause> = <whereclause> + ' OR ' +  <whereclause>
<whereclause> = 'NOT' + <whereclause>

\{ExecuteError\} MakeFeatureLayer\_management(<fc>,<newlayer>)

\{ExecuteError\} MakeFeatureLayer\_management(<fc>,<newlayer>,where\_clause=<whereclause>)
\end{code}
}
\caption{A small TSTL file to test one ArcPy function.}
\label{fig:makefeaturelayer}
\end{figure}

\begin{figure}
{\scriptsize
\begin{code}
import sut, random, time
rgen = random.Random()
sut = sut.sut()
NUM\_TESTS = 1000
TEST\_LENGTH = 200 
for t in xrange(0,NUM\_TESTS):
   sut.restart()
   for s in xrange(0,TEST\_LENGTH): 
       action = sut.randomEnabled(rgen)
       r = sut.safely(action)
       if len(sut.newBranches()) > 0:
          print time.time(),'NEW BRANCHES:', sut.newBranches()
       if (not r) or (not sut.check()):
          pred = sut.failsCheck if r else sut.fails
          print 'TEST FAILED:', sut.error() 
          R = sut.reduce(sut.test(), pred)
          N = sut.normalize(R, pred) 
          sut.generalize(N,pred)
\end{code}
}
\caption{A simple random tester using the interface provided by TSTL.}
\label{fig:rt}
\end{figure}

Figure \ref{fig:makefeaturelayer} shows a simple TSTL harness for the
function documented in Figure \ref{fig:MakeFeatureLayer}.  Even this
short harness supports constructing SQL where clauses of arbitrary
length and selecting field names based on data files.  Figure
\ref{fig:rt} shows a simple pure random test generator that can test
any SUT (including this one) with a TSTL-defined harness.  This
harness, in 20 lines of code, not only provides automated test
generation, but continuous reporting of incremental branch coverage,
delta-debugging \cite{DD} for reduction of failing tests, and
additional TSTL-specific post-processing that further reduces the size
and complexity of test cases for debugging.  The brevity of the test
generator, no matter how complex the SUT, is made possible by the
common functionality of all TSTL-generated testing interfaces.  The
TSTL compiler produces a Python (or other target language) class that allows a test generation or manipulation
tool to view a testing problem as exploration of a (possibly infinite)
graph of states.  Transitions in the graph are the available test actions, executed in the underlying language, and are guarded by both TSTL
restrictions on the semantics of valid tests and user-defined guards
on system behavior.  States include both the (possibly unknown) state
of the SUT and the TSTL state, including pools of values to be used in actions.

In this simple example, the only ``oracle'' is the implicit property
that the system should neither crash nor raise an unexpected
exception.  For testing many systems, this is sufficient: we have
discovered real bugs in many Python libraries with only this level of
checking, similar to most of what a tool such as Randoop
\cite{Pacheco} or JCrasher \cite{CsallnerS04} checks.  TSTL also
checks arbitrary assertions/invariants defined in the language of the
SUT, supporting tradtional property-based testing
\cite{ClaessenH00,hypothesis} (described in Section \ref{sec:property}).  Finally, TSTL includes sophisticated
support for differential testing \cite{Differential,ICSEDiff}, where a
system is tested with respect to the behavior of a reference library.
TSTL makes it easy to wrap a reference system to account for expected
differences, and supports partial reference testing (see Section \ref{sec:differential}).

\section{Motivating Case Study: Esri ArcPy}
\label{sec:arcpy}

Esri is the single largest GIS software vendor, with about 40\% of
global market share.  Esri's ArcGIS tools are extremely widely used
for GIS analysis, in government, scientific research, commercial
enterprises, and education.  Automation of complex GIS analysis and
data management is essential, and Esri has long provided tools for
programming their GIS software tools.  The newest such method,
introduced in ArcGIS 10.0, is a Python site-package, ArcPy
\cite{ArcPy}.  ArcPy is a complex library, with dozens of classes and
hundreds of functions distributed over a variety of of toolboxes.
Most of the code executed in carrying out ArcPy functions is the code
for the ArcGIS engine itself.  This source code, written in C++
(amounting to millions of lines), is
not available.  The source code for the latest version (10.3) of the
Python site-package alone, however, which interfaces with the ArcGIS
engine, is over 50,000 lines of code.  This is a very large system
(especially given the compactness of Python code), comparable in size
to the largest software systems previously tested using automated test
generation, such as core Java and Apache libraries
\cite{FA11,Pacheco}.

In order to improve the reliability of ArcPy, we are developing a
framework for automated testing of ArcPy itself, as well as libraries
based on ArcPy.  The TSTL harness for ArcPy is already more than six times as
large as the next-largest such definition previously implemented in
TSTL, even though the harness so far only includes a small portion of ArcPy
API (Application Program Interface) calls. The first stage of testing has resulted in discovery of
multiple faults in ArcPy/ArcGIS, and has required modifications to the
TSTL language and, especially, to the tool chain supporting test replay,
debugging, and test case understanding.

%One of the contributions of this paper is a more in-depth discussion
%of the problems, challenges, and tool utility aspects of testing
%software than is typical in most research papers in the field.  Such
%papers are (understandably) typically focused on novel algorithms or
%empirical evaluations of known methods, rather than the practical aspects of finding
%and understanding faults in a real-world software system.


%\subsection{Automated Testing for the Rest of the World}

Previous work on automated test generation for APIs has been largely carried
out by software testing researchers only, or (at most) by software
testing researchers working with individuals who are primarily
software developers.  This paper describes TSTL in the context of a
testing effort largely directed (and coded) by the first author, who is not a
software developer by profession or education, but a GIS analyst.
The problem of end-user testing
\cite{burnettEUSE,Silos,rothermelTOSEM} is long-standing.  Previous
work in the field has often focused on
non-traditional programming: e.g. spreadsheets
\cite{rothermelTOSEM}, visual languages, or machine-learning systems
\cite{OnlyOracle}.  TSTL is partly designed to allow a user who is
familiar with a software library but not expert in software testing
techniques to test a traditional software API library.  In one sense,
this is a less difficult scenario than spreadsheets
or visual forms, in that the testing is directed by an individual used
to writing and thinking about code.  The concepts in
automated software testing are most easily understood by those
who are also familiar with a conventional
programming language.  On the other hand, ArcPy is
not a small user-developed program but a large, complex system.
ArcPy was also not written by the end-user, or by any of the authors
of this paper, nor have the authors received any assistance in the
effort from Esri.

Automated testing systems more advanced than a simple hand-written
loop generating a few random inputs to a handful of functions, or more
complicated to use than a fully push-button system are often
considered too difficult for practical use even by software developers
or software QA staff \cite{ISSRE12}. Even ``push-button'' tools for
automated testing are sometimes difficult for expert users to install,
apply, and configure \cite{AMAI,CFV08,ISSRE12}.  TSTL aims to be
relatively easy to use for anyone familiar with basic Python
development.  By avoiding the use of a toy problem and presenting TSTL
in the context of a more typical real-world system (vs. e.g., a simple
container class), we hope to make it easier to apply to other
real-world systems.
%One goal of this work
%has been to mature the TSTL language and tool chain so Python
%programmers from all backgrounds can easily apply it to their
%automated testing problems, with tools approximately as easy to use as
%other Python developer tools.

%The second major contribution of this paper is therefore a presentation of an
%approach to automated testing that has been chosen by a GIS analyst, not a
%software developer or testing researcher.  Moreover, we present this
%paper as a proof-of-concept that modern automated testing, even in a
%highly interactive, non-push-button form, can be used by a motivated
%domain expert, with the support of  a domain-specific language
%\cite{Fow10} and a set of tools for generating, analyzing, and replaying tests.


