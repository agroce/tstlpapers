\section{Introduction}

Automated test generation tools, in essence, exist in order to produce
failing tests.  However, once a tool has produced either one such test
or a large set of such tests, the real work of a user of such a tool
begins in earnest.  First, users of course wish to be able to replay
failed tests, and run batches of tests as regressions (possibly
collecting code coverage information as well).  If the test format of
a tool is not executable, it is also useful to be able to produce
executable, standalone tests.   Second, with most test generation methods, it is
important to reduce the size of the test to make it easier to
understand (and quicker to run as a regression test)
\cite{DD,MinUnit,TCminim,ICSEDiff,icst2014}.  Most industrial-strength
automated testing systems support test minimization, usually using a
variation of delta-debugging \cite{CReduce,hypothesis,lithium}.  Some
such systems make it easy to customize the criteria by which a test is reduced.
However, such systems usually do not provide algorithmic defenses
against the problem of slippage (where, in the absence of a good
failure labeling system, the reducer may change a failure due to one
fault to a failure due to another, and often less interesting, fault \cite{PLDI13,slippage}).
Additionally, a user may want to \emph{semantically} simplify a test, to
make it not only shorter but simpler and less complex in ways that go
beyond mere test length \cite{OneTest}.  Such functionality is less common, or only a
byproduct of specialized minimization methods.  Finally, users may
want to be informed of the neighborhood of a failing test:  which
similar tests also fail (and, equally important, which similar tests
do not fail) \cite{OneTest}.  Again, this functionality is usually not present in
current automated test generation tools.

The TSTL testing language and tool, currently implemented for testing
Python programs, supports all of these features, both as command-line
tools and API interfaces for more complex uses.

\section{A Brief TSTL Primer}

TSTL \cite{tstlsttt,NFM15,ISSTA15} is a language, tool suite, and
``library constructor'' for testing Python programs.  TSTL aims to
offer both the immediate feedback of property-driven testing tools
like QuickCheck and Hypothesis \cite{ClaessenH00,hypothesis} and
longer-term automated test generation, as well as serve as a platform
for experimenting with novel test generation and test manipulation
methods.   Unlike most QuickCheck-like tools, TSTL is focused on
generating unit tests that consist of sequences of method/function
calls \cite{AndrewsTR}, rather than generating input data for functions (though TSTL
can generate arbitrary data, since data creation is usually easily
expressed as a sequence of constructions and modifications of data).
TSTL is available on github at
\url{https://github.com/agroce/tstl}.  Simply typing {\tt pip
  install tstl} on a system with pip installed will also install TSTL.

The user of TSTL writes a test harness \cite{WODACommon} that describes
the actions that are possible during testing, and the pools of values
that are generated during testing (these are both the inputs to
methods tested and the objects to be tested, in most cases).  Figure
\ref{fig:stack} shows a simple TSTL harness to test a Python stack
implementation.  The harness creates integer values (in the range
1-20) and stacks, up to 4 of each.  It calls the various methods of
the stack.  For stack operations that can cause underflow, there is
both a version of the action that guards the action with a check on
stack emptiness and a version that throws away the return value and
allows the call to throw an {\tt IndexError} exception.  TSTL also
supports automatic differential \cite{Differential} testing, where one implementation
serves as a reference model for the Software Under Test, and other
features.  See the journal paper on TSTL \cite{tstlsttt} or the online
documentation on github for more details.

\begin{figure}
{\scriptsize
\begin{code}
@import stack
\vspace{0.1in}
pool: <stack> 4        \# These are the variables used in tests,
pool: <value> 4        \# stack0...stack3 and value0...value3
\vspace{0.1in}
<value> := <[1..20]>       \# := is Python assignment, but
<stack> := stack.Stack()   \# marks a value  as initialized
\vspace{0.1in}
<stack>.push(<value>) \# This is a test action
\{IndexError\} <value> := <stack>.pop() \# ... as are all these
not <stack,1>.isEmpty() -> <value> := <stack>.pop()
\{IndexError\} <value> := <stack>.peek() 
not <stack,1>.isEmpty() -> <value> := <stack>.peek()
\end{code}
}
\caption{A simple TSTL harness for a stack.}
\label{fig:stack}
\end{figure}

To make use of the stack harness, we save it into a file, such
as {\tt stack.tstl}.  Then at a command prompt, we compile the file into
a standalone interface for testing the stack, {\tt sut.py}, and
(usually) invoke the basic TSTL testing tool, {\tt tstl\_rt} to look for faults.

{\scriptsize
\begin{code}
 > tstl stack.tstl
 > tstl\_rt --timeout 30
\end{code}
}

If we forget how to use the tools, all TSTL command line tools
produce a full list of options when called with the {\tt --help}
argument.  For most tools this list is short and simple; the ``random
tester'' {\tt tstl\_rt}, however, has a very large number of options,
since it supports pure random testing, swarm testing \cite{ISSTA12},
genetic-algorithm based testing, control over action probabilities,
Markov-model driven testing, and a large array of other configuration settings.

TSTL generates 738 tests (of 100 operations each) and performs
73,749 actions.  In this case, there is no fault to be found.  This
paper describes the tools TSTL provides for working with tests in the
instance when a fault is detected.
