Test case reduction has long been seen as essential to effective automated testing.  However, test case reduction simply reduces the length of a test case.  It does not attempt to produce \emph{semantic} simplicity.  In this paper, we present two algorithms.  The first provides simplification for test-cases by rewriting them into a \emph{normal form}.  Test cases converted into normal form are sometimes shorter than the test cases produced by delta-debugging, but the primary feature of normalization is that it often converts many test cases that expose the same fault into \emph{a single test}, reducing the number of test cases that a reader must examine, and partially addressing the ``fuzzer taming'' problem of determining how many faults are present in a set of failing test cases.  Generalization, in contrast, takes a test and reports what aspects of the test could have been changed while preserving the property that the test fails.  Together, ideally, normalization plus generalization allows a user to replace reading a set of test cases that vary in unimportant ways with reading \emph{one annotated test case} per fault.  While falling short, our approach is a significant step towards this goal.  Our algorithms rely on the features of a recently introduced DSL for testing.   We show how normalization plus generalization aids understanding of test cases, including tests for a complex and widely used commercial GIS library.  Normalization can often reduce (by well over an order of magnitude) the number of different test cases to be examined, without loss of fault detection.